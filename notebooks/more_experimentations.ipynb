{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549d7b42e1d434cb",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349c8478b5b56ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:42:07.647108Z",
     "start_time": "2025-06-03T13:42:06.602877Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704dbc60eb67e2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:42:10.781526Z",
     "start_time": "2025-06-03T13:42:09.965116Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.optimizers.LagrangianTrainer import LagrangianTrainer\n",
    "from src.optimizers.SimpleTrainer import SimpleTrainer\n",
    "from src.utils import dataset\n",
    "from src.optimizers.HypercubeTrainer import HypercubeTrainer\n",
    "from src.utils.evaluation import evaluate_accuracy\n",
    "from src.cert import Safebox\n",
    "\n",
    "from src.utils.dataset import reduce_dataset\n",
    "from src.optimizers.SAMTrainer import SAMTrainer\n",
    "from src.utils.LabelNoiseDataset import LabelNoiseDataset\n",
    "\n",
    "from src.optimizers.volumes import LogVolume\n",
    "from src.optimizers.volumes import MinSideLengthVolume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6b7dd2a0953cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:42:12.492007Z",
     "start_time": "2025-06-03T13:42:12.437064Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894def22ea202f",
   "metadata": {},
   "source": [
    "# Tests on FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a7be3cf9b5c212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:21.685873Z",
     "start_time": "2025-06-02T14:49:21.484854Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.get_fashion_mnist_dataset()\n",
    "train_dataset = reduce_dataset(train_dataset, num_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1930dff7094eba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:42:21.556406Z",
     "start_time": "2025-06-03T13:42:21.538293Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(output_dim=10):\n",
    "    \"\"\"Returns a simple CNN model.\"\"\"\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 24, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(24, 8, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(8, 1, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(676, 676),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(676, output_dim),\n",
    "    ).to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f715ec6464108",
   "metadata": {},
   "source": [
    "### Label Noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac50cd2609a99d00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:42:15.947883Z",
     "start_time": "2025-06-03T13:42:15.872133Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.get_fashion_mnist_dataset()\n",
    "train_dataset = LabelNoiseDataset(train_dataset, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155dc4594e502d3",
   "metadata": {},
   "source": [
    "##### Adam\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90ee42b99e99df69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:42:20.941092Z",
     "start_time": "2025-06-03T12:35:42.454017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [06:38<00:00, 50.19it/s, loss=0.0426, val_acc=0.766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = get_model()\n",
    "\n",
    "base_trainer = SimpleTrainer(base_model, device=DEVICE)\n",
    "base_model = base_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=20000, batch_size=64, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5bd551cd4510ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:43:08.857147Z",
     "start_time": "2025-06-03T12:43:06.329542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model accuracy 0.9700999855995178\n",
      "Validation base model accuracy 0.7691999673843384\n"
     ]
    }
   ],
   "source": [
    "print(\"Training base model accuracy\", evaluate_accuracy(train_dataset, base_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation base model accuracy\", evaluate_accuracy(val_dataset, base_model, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54791a7968d2ac93",
   "metadata": {},
   "source": [
    "##### Sam rho = 0.05\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "548909ec1667487c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T11:53:23.809524Z",
     "start_time": "2025-06-03T11:46:53.238858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [06:30<00:00, 51.21it/s, loss=0.0558, val_acc=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    }
   ],
   "source": [
    "sam_model = get_model()\n",
    "\n",
    "sam_trainer = SimpleTrainer(sam_model, device=DEVICE)\n",
    "sam_model = sam_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=20000, batch_size=64, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a48641cbfe451b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:00:35.796751Z",
     "start_time": "2025-06-03T12:00:33.395093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model accuracy 0.9751999974250793\n",
      "Validation base model accuracy 0.7558000087738037\n"
     ]
    }
   ],
   "source": [
    "print(\"Training sam model accuracy\", evaluate_accuracy(train_dataset, sam_model, num_samples=len(train_dataset), device=DEVICE))\n",
    "print(\"Validation sam model accuracy\", evaluate_accuracy(val_dataset, sam_model, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30307700b8d72aee",
   "metadata": {},
   "source": [
    "##### Sam rho = 0.1\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d38267f58b4b0c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:08:28.433659Z",
     "start_time": "2025-06-03T12:01:56.657858Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [06:31<00:00, 51.05it/s, loss=0.106, val_acc=0.766] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sam_model_1 = get_model()\n",
    "\n",
    "sam_trainer_1 = SimpleTrainer(sam_model_1, device=DEVICE)\n",
    "sam_model_1 = sam_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=20000, batch_size=64, lr=1e-3, rho=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ab4b1669f94119c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:10:20.849950Z",
     "start_time": "2025-06-03T12:10:18.437832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sam model accuracy 0.9876999855041504\n",
      "Validation sam model accuracy 0.7486000061035156\n"
     ]
    }
   ],
   "source": [
    "print(\"Training sam model accuracy\", evaluate_accuracy(train_dataset, sam_model_1, num_samples=len(train_dataset), device=DEVICE))\n",
    "print(\"Validation sam model accuracy\", evaluate_accuracy(val_dataset, sam_model_1, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbd0c01e0c4f7a",
   "metadata": {},
   "source": [
    "##### Hypercube 1e-6\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e95296ade5ce52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:34:50.663056Z",
     "start_time": "2025-06-03T12:25:09.683693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [09:40<00:00, 34.43it/s, loss=0.145, min_val_acc=0.812] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hypercube_model = get_model()\n",
    "\n",
    "hypercube_trainer = HypercubeTrainer(hypercube_model, device=DEVICE)\n",
    "\n",
    "hypercube_trainer.set_volume_constrain(1e-6) # start with a small volume at first\n",
    "hypercube_trainer.train(\n",
    "    train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=20000, batch_size=64, lr=1e-3\n",
    ")\n",
    "hypercube_model = Safebox.bmodelToModel(hypercube_trainer.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bccc616a88fd5fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:35:21.068796Z",
     "start_time": "2025-06-03T12:35:16.735215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training hypercube accuracy  0.9750999808311462\n",
      "Validation hypercube accuracy  0.7750999927520752\n"
     ]
    }
   ],
   "source": [
    "print(\"Training hypercube accuracy \", evaluate_accuracy(train_dataset, hypercube_model, num_samples=len(val_dataset)))\n",
    "print(\"Validation hypercube accuracy \", evaluate_accuracy(val_dataset, hypercube_model, num_samples=len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b432d7d79346a",
   "metadata": {},
   "source": [
    "##### Hypercube 2e-6\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bfe24293ad7b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:55:12.181269Z",
     "start_time": "2025-06-03T12:45:23.109787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [09:49<00:00, 33.95it/s, loss=0.29, min_val_acc=0.781]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hypercube_model_1 = get_model()\n",
    "\n",
    "hypercube_trainer_1 = HypercubeTrainer(hypercube_model_1, device=DEVICE)\n",
    "\n",
    "hypercube_trainer_1.set_volume_constrain(2e-6) # start with a small volume at first\n",
    "hypercube_trainer_1.train(\n",
    "    train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=20000, batch_size=64, lr=1e-3\n",
    ")\n",
    "hypercube_model_1 = Safebox.bmodelToModel(hypercube_trainer_1.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9563c9e71a53a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:56:10.707946Z",
     "start_time": "2025-06-03T12:56:06.512932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training hypercube accuracy  0.9448999762535095\n",
      "Validation hypercube accuracy  0.7612000107765198\n"
     ]
    }
   ],
   "source": [
    "print(\"Training hypercube accuracy \", evaluate_accuracy(train_dataset, hypercube_model_1, num_samples=len(val_dataset)))\n",
    "print(\"Validation hypercube accuracy \", evaluate_accuracy(val_dataset, hypercube_model_1, num_samples=len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa239f529a9111e",
   "metadata": {},
   "source": [
    "##### Hypercube 3.5e-6\n",
    "(anything above simply wouldn't train)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7fd64d274501652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:09:01.239374Z",
     "start_time": "2025-06-03T12:59:20.108295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [09:41<00:00, 34.42it/s, loss=0.22, min_val_acc=0.688]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hypercube_model_2 = get_model()\n",
    "\n",
    "hypercube_trainer_2 = HypercubeTrainer(hypercube_model_2, device=DEVICE)\n",
    "\n",
    "hypercube_trainer_2.set_volume_constrain(3.5e-6) # start with a small volume at first\n",
    "hypercube_trainer_2.train(\n",
    "    train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=20000, batch_size=64, lr=1e-3\n",
    ")\n",
    "hypercube_model_2 = Safebox.bmodelToModel(hypercube_trainer_2.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dda594659dbbf5b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:09:46.869352Z",
     "start_time": "2025-06-03T13:09:42.648085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training hypercube accuracy  0.9430999755859375\n",
      "Validation hypercube accuracy  0.7695000171661377\n"
     ]
    }
   ],
   "source": [
    "print(\"Training hypercube accuracy \", evaluate_accuracy(train_dataset, hypercube_model_2, num_samples=len(val_dataset)))\n",
    "print(\"Validation hypercube accuracy \", evaluate_accuracy(val_dataset, hypercube_model_2, num_samples=len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5817feefcef1cb3",
   "metadata": {},
   "source": [
    "##### Lagrangian logVolume\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ec3b7cbbecc5a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:48:37.571252Z",
     "start_time": "2025-06-03T14:48:15.233938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.517036437988281\n",
      "tensor([-8.5170], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 556/3000 [00:22<01:37, 25.09it/s, loss=1, min_val_acc=0.0312, current_volume=-4.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m lagrangian_trainer.set_volume_constrain(\u001b[32m1e-4\u001b[39m) \u001b[38;5;66;03m# start with a small volume at first\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(lagrangian_trainer._volume_function(lagrangian_trainer._interval_model))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mlagrangian_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m0.000000000000001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:16\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:33\u001b[39m, in \u001b[36mConstrainedVolumeTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m._interval_model.train()\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m._current_val_dataset=val_dataset\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/Trainer.py:30\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m     X, y = \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[32m     29\u001b[39m X, y = X.to(\u001b[38;5;28mself\u001b[39m._device), y.to(\u001b[38;5;28mself\u001b[39m._device)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m loss, info_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m progress_bar.set_postfix({\n\u001b[32m     32\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(loss, \u001b[32m4\u001b[39m),\n\u001b[32m     33\u001b[39m                          } | info_dict)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss < loss_obj:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:59\u001b[39m, in \u001b[36mConstrainedVolumeTrainer.step\u001b[39m\u001b[34m(self, X, y, lr, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m._interval_model.train()\n\u001b[32m     58\u001b[39m loss, infos = \u001b[38;5;28mself\u001b[39m._optimize_step(X, y, lr=lr, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m min_acc = \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_min_val_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_current_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[32m4\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, {\u001b[33m\"\u001b[39m\u001b[33mmin_val_acc\u001b[39m\u001b[33m\"\u001b[39m: min_acc} | infos\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:46\u001b[39m, in \u001b[36mConstrainedVolumeTrainer._evaluate_min_val_acc\u001b[39m\u001b[34m(self, val_dataset, num_samples)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate_min_val_acc\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     44\u001b[39m                           val_dataset: torch.utils.data,\n\u001b[32m     45\u001b[39m                           num_samples: \u001b[38;5;28mint\u001b[39m = \u001b[32m64\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     X, y = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m._interval_model.eval()\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:908\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize\u001b[39m(tensor: Tensor, mean: List[\u001b[38;5;28mfloat\u001b[39m], std: List[\u001b[38;5;28mfloat\u001b[39m], inplace: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Tensor:\n\u001b[32m    906\u001b[39m     _assert_image_tensor(tensor)\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    909\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput tensor should be a float tensor. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tensor.ndim < \u001b[32m3\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#lagrangian_model = get_model()\n",
    "#lagrangian_trainer = LagrangianTrainer(lagrangian_model, LogVolume(device=DEVICE), device=DEVICE)\n",
    "#lagrangian_trainer.set_volume_constrain(1e-4) # start with a small volume at first\n",
    "#print(lagrangian_trainer._volume_function(lagrangian_trainer._interval_model))\n",
    "#lagrangian_trainer.train(\n",
    "#    train_dataset, val_dataset, loss_obj=-0.000000000000001, max_iters=3000, batch_size=64, lr=1e-3\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0ee8393a27092",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "Sam and Hypercube don't provide better generalization properties than Adam when trained on swapped labels\n",
    "\n",
    "Actually they both perform way worse than early stopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63567bfdcab6e876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c6cd9b2435910bd",
   "metadata": {},
   "source": [
    "# Tests on Ciphar 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31a99385abff9162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:28:04.546757Z",
     "start_time": "2025-06-02T15:28:03.279350Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = dataset.get_cifar100_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc4bc37f3835e599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:29:47.692976Z",
     "start_time": "2025-06-02T15:29:47.672348Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(output_dim=100):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(3, 12, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(12, 48, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(48, 12, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(12, 1, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(784,784),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(784, output_dim),\n",
    "    ).to(DEVICE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ac735091b00a6",
   "metadata": {},
   "source": [
    "##### Baseline with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbedb141604afab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:33:22.225770Z",
     "start_time": "2025-06-02T15:29:49.364404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:32<00:00, 46.99it/s, loss=0.406, val_acc=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    }
   ],
   "source": [
    "base_model = get_model()\n",
    "\n",
    "base_trainer = SimpleTrainer(base_model, device=DEVICE)\n",
    "base_model = base_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b692be0391f31301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:33:33.345423Z",
     "start_time": "2025-06-02T15:33:30.375879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model accuracy 0.9325999617576599\n",
      "Validation base model accuracy 0.19429999589920044\n"
     ]
    }
   ],
   "source": [
    "print(\"Training base model accuracy\", evaluate_accuracy(train_dataset, base_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation base model accuracy\", evaluate_accuracy(val_dataset, base_model, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2532908864df1e",
   "metadata": {},
   "source": [
    "##### Sam\n",
    "-------------------------\n",
    "\n",
    "rho = 0.05 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7728800ee10035ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:37:41.494993Z",
     "start_time": "2025-06-02T15:33:45.884597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:55<00:00, 42.45it/s, loss=0.987, val_acc=0.0781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sam_model = get_model()\n",
    "\n",
    "sam_trainer = SAMTrainer(sam_model, device=DEVICE)\n",
    "sam_model = sam_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c43aa78adab78017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:19:14.922911Z",
     "start_time": "2025-06-02T16:19:11.965003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy 0.9426999688148499\n",
      "Validation SAM model accuracy 0.18490000069141388\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SAM model accuracy\", evaluate_accuracy(train_dataset, sam_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy\", evaluate_accuracy(val_dataset, sam_model, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c453579e3c12f0",
   "metadata": {},
   "source": [
    "rho = 0.01 (smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97c87129620d8e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:10:30.526015Z",
     "start_time": "2025-06-02T21:06:33.829528Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:56<00:00, 42.26it/s, loss=0.512, val_acc=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sam_model_1 = get_model()\n",
    "\n",
    "sam_trainer_1 = SAMTrainer(sam_model_1, device=DEVICE)\n",
    "sam_model_1 = sam_trainer_1.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9c677aa353e57d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:10:33.704521Z",
     "start_time": "2025-06-02T21:10:30.595925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy (rho=0.01) 0.9380999803543091\n",
      "Validation SAM model accuracy (rho=0.01) 0.19029998779296875\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SAM model accuracy (rho=0.01)\", evaluate_accuracy(train_dataset, sam_model_1, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy (rho=0.01)\", evaluate_accuracy(val_dataset, sam_model_1, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbc14c993c9365",
   "metadata": {},
   "source": [
    "rho = 0.001 (way smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27eea8974f740326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:14:30.830365Z",
     "start_time": "2025-06-02T21:10:33.713736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:57<00:00, 42.18it/s, loss=0.373, val_acc=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sam_model_2 = get_model()\n",
    "\n",
    "sam_trainer_2 = SAMTrainer(sam_model_2, device=DEVICE)\n",
    "sam_model_2 = sam_trainer_2.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86e1c1038b13c8c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:14:33.870277Z",
     "start_time": "2025-06-02T21:14:30.872448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy (rho=0.001) 0.9375999569892883\n",
      "Validation SAM model accuracy (rho=0.001) 0.18809999525547028\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SAM model accuracy (rho=0.001)\", evaluate_accuracy(train_dataset, sam_model_2, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy (rho=0.001)\", evaluate_accuracy(val_dataset, sam_model_2, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244c74e596bbc5f",
   "metadata": {},
   "source": [
    "rho = 0.1 (bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51d987135c1bf3b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:18:30.776437Z",
     "start_time": "2025-06-02T21:14:33.879737Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:56<00:00, 42.22it/s, loss=2.16, val_acc=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  2 ----------\n"
     ]
    }
   ],
   "source": [
    "sam_model_3 = get_model()\n",
    "\n",
    "sam_trainer_3 = SAMTrainer(sam_model_3, device=DEVICE)\n",
    "sam_model_3 = sam_trainer_3.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42ea1ca335c85a6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:18:52.039792Z",
     "start_time": "2025-06-02T21:18:48.978488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy (rho=0.1) 0.7524999976158142\n",
      "Validation SAM model accuracy (rho=0.1) 0.18159998953342438\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SAM model accuracy (rho=0.1)\", evaluate_accuracy(train_dataset, sam_model_3, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy (rho=0.1)\", evaluate_accuracy(val_dataset, sam_model_3, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28540ed2893ccb3f",
   "metadata": {},
   "source": [
    "### Vanilla SGD vs SAM + SGD\n",
    "-----------------------------\n",
    "\n",
    "Not modularized yet (changed manually adam -> sgd in SamTrainer and SimpleTrainer :/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47bc3d3d1553ee78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:24:15.737721Z",
     "start_time": "2025-06-02T21:20:20.966826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:54<00:00, 42.60it/s, loss=4.6, val_acc=0.0312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  5 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sam_sgd_model = get_model()\n",
    "\n",
    "sam_sgd_trainer = SAMTrainer(sam_sgd_model, device=DEVICE)\n",
    "sam_sgd_model = sam_sgd_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b68f2c66eee16a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:24:18.792746Z",
     "start_time": "2025-06-02T21:24:15.772268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD SAM model accuracy (rho=0.01) 0.012600000016391277\n",
      "Validation SGD SAM model accuracy (rho=0.01) 0.013799999840557575\n"
     ]
    }
   ],
   "source": [
    "print(\"Training SGD SAM model accuracy (rho=0.01)\", evaluate_accuracy(train_dataset, sam_sgd_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SGD SAM model accuracy (rho=0.01)\", evaluate_accuracy(val_dataset, sam_sgd_model, num_samples=len(val_dataset), device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57c2e1f7a0427dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:33:11.281235Z",
     "start_time": "2025-06-02T21:32:49.265386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1058/10000 [00:21<03:04, 48.43it/s, loss=4.61, val_acc=0.0156]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m base_model = get_model()\n\u001b[32m      3\u001b[39m base_trainer = SimpleTrainer(base_model, device=DEVICE)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m base_model = \u001b[43mbase_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.000000000000001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:5\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/Trainer.py:30\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m     X, y = \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[32m     29\u001b[39m X, y = X.to(\u001b[38;5;28mself\u001b[39m._device), y.to(\u001b[38;5;28mself\u001b[39m._device)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m loss, info_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m progress_bar.set_postfix({\n\u001b[32m     32\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(loss, \u001b[32m4\u001b[39m),\n\u001b[32m     33\u001b[39m                          } | info_dict)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss < loss_obj:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/SimpleTrainer.py:48\u001b[39m, in \u001b[36mSimpleTrainer.step\u001b[39m\u001b[34m(self, X, y, lr, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m loss.backward()\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item(), {\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m:\u001b[38;5;28mround\u001b[39m(accuracy, \u001b[32m4\u001b[39m)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/SimpleTrainer.py:36\u001b[39m, in \u001b[36mSimpleTrainer._evaluate_accuracy\u001b[39m\u001b[34m(self, num_samples)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m._model.eval()\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/utils/evaluation.py:8\u001b[39m, in \u001b[36mevaluate_accuracy\u001b[39m\u001b[34m(dataset, model, num_samples, device)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_accuracy\u001b[39m(dataset: torch.utils.data.Dataset, model: torch.nn.Sequential, num_samples: \u001b[38;5;28mint\u001b[39m = \u001b[32m2000\u001b[39m,\n\u001b[32m      7\u001b[39m                       device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     X, y = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     X, y = X.to(device), y.to(device)\n\u001b[32m     10\u001b[39m     logit = model(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/datasets/cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m std.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    927\u001b[39m     std = std.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "base_model = get_model()\n",
    "\n",
    "base_trainer = SimpleTrainer(base_model, device=DEVICE)\n",
    "base_model = base_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421b9889cf7788c",
   "metadata": {},
   "source": [
    "##### Lagrangian\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49eb57ecd2bffc9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:32:41.003512Z",
     "start_time": "2025-06-02T16:31:45.808228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.5172, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1060/3000 [00:54<01:40, 19.30it/s, loss=4.6, min_val_acc=0, current_volume=-6.9]     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m lagrangian_trainer.set_volume_constrain(\u001b[32m1e-4\u001b[39m) \u001b[38;5;66;03m# start with a small volume at first\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(lagrangian_trainer._volume_function(lagrangian_trainer._interval_model))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mlagrangian_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m0.000000000000001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:16\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:33\u001b[39m, in \u001b[36mConstrainedVolumeTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m._interval_model.train()\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m._current_val_dataset=val_dataset\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/Trainer.py:30\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m     X, y = \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[32m     29\u001b[39m X, y = X.to(\u001b[38;5;28mself\u001b[39m._device), y.to(\u001b[38;5;28mself\u001b[39m._device)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m loss, info_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m progress_bar.set_postfix({\n\u001b[32m     32\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(loss, \u001b[32m4\u001b[39m),\n\u001b[32m     33\u001b[39m                          } | info_dict)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss < loss_obj:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:59\u001b[39m, in \u001b[36mConstrainedVolumeTrainer.step\u001b[39m\u001b[34m(self, X, y, lr, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m._interval_model.train()\n\u001b[32m     58\u001b[39m loss, infos = \u001b[38;5;28mself\u001b[39m._optimize_step(X, y, lr=lr, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m min_acc = \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_min_val_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_current_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[32m4\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, {\u001b[33m\"\u001b[39m\u001b[33mmin_val_acc\u001b[39m\u001b[33m\"\u001b[39m: min_acc} | infos\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:46\u001b[39m, in \u001b[36mConstrainedVolumeTrainer._evaluate_min_val_acc\u001b[39m\u001b[34m(self, val_dataset, num_samples)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate_min_val_acc\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     44\u001b[39m                           val_dataset: torch.utils.data,\n\u001b[32m     45\u001b[39m                           num_samples: \u001b[38;5;28mint\u001b[39m = \u001b[32m64\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     X, y = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m._interval_model.eval()\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/datasets/cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m std.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    927\u001b[39m     std = std.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m.div_(std)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lagrangian_model = get_model()\n",
    "lagrangian_trainer = LagrangianTrainer(lagrangian_model, LogVolume(epsilon=1e-12), device=DEVICE)\n",
    "lagrangian_trainer.set_volume_constrain(1e-4) # start with a small volume at first\n",
    "print(lagrangian_trainer._volume_function(lagrangian_trainer._interval_model))\n",
    "lagrangian_trainer.train(\n",
    "    train_dataset, val_dataset, loss_obj=-0.000000000000001, max_iters=3000, batch_size=64, lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3741be3d426b6981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:25:07.502587Z",
     "start_time": "2025-06-02T16:25:04.468891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model accuracy 0.010300000198185444\n",
      "Validation base model accuracy 0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "print(\"Training base model accuracy\",\n",
    "      evaluate_accuracy(train_dataset, lagrangian_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation base model accuracy\",\n",
    "      evaluate_accuracy(val_dataset, lagrangian_model, num_samples=len(val_dataset), device=DEVICE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
