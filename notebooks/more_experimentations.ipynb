{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "549d7b42e1d434cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:14.273700Z",
     "start_time": "2025-06-02T14:49:11.549018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.optimizers.SAMTrainer import SAMTrainer\n",
    "from src.optimizers.volumes import VolumeFunction, LogVolume\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ],
   "id": "d349c8478b5b56ff",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:19:28.471399Z",
     "start_time": "2025-06-02T21:19:28.451010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from src.optimizers.LagrangianTrainer import LagrangianTrainer\n",
    "from src.optimizers.SimpleTrainer import SimpleTrainer\n",
    "from src.utils import dataset\n",
    "from src.optimizers.HypercubeTrainer import HypercubeTrainer\n",
    "from src.utils.evaluation import evaluate_accuracy\n",
    "from src.cert import Safebox\n",
    "\n",
    "from src.utils.dataset import reduce_dataset\n",
    "\n",
    "from src.optimizers.volumes import LogVolume\n"
   ],
   "id": "704dbc60eb67e2b4",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:19.670703Z",
     "start_time": "2025-06-02T14:49:19.565215Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "id": "5e6b7dd2a0953cc6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tests on FashionMNIST",
   "id": "b894def22ea202f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:21.685873Z",
     "start_time": "2025-06-02T14:49:21.484854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset, val_dataset = dataset.get_fashion_mnist_dataset()\n",
    "train_dataset = reduce_dataset(train_dataset, num_samples=300)"
   ],
   "id": "90a7be3cf9b5c212",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:23.523087Z",
     "start_time": "2025-06-02T14:49:23.486077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model(output_dim=10):\n",
    "    \"\"\"Returns a simple CNN model.\"\"\"\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(8, 1, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(576, output_dim),\n",
    "    ).to(DEVICE)\n",
    "    return model"
   ],
   "id": "1930dff7094eba4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tests on Ciphar 100\n",
    "\n"
   ],
   "id": "8c6cd9b2435910bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:28:04.546757Z",
     "start_time": "2025-06-02T15:28:03.279350Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, val_dataset = dataset.get_cifar100_dataset()",
   "id": "31a99385abff9162",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:29:47.692976Z",
     "start_time": "2025-06-02T15:29:47.672348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model(output_dim=100):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(3, 12, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(12, 48, kernel_size=5, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(48, 12, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(12, 1, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(784,784),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(784, output_dim),\n",
    "    ).to(DEVICE)\n",
    "    return model"
   ],
   "id": "bc4bc37f3835e599",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Baseline with Adam",
   "id": "ef7ac735091b00a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:33:22.225770Z",
     "start_time": "2025-06-02T15:29:49.364404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = get_model()\n",
    "\n",
    "base_trainer = SimpleTrainer(base_model, device=DEVICE)\n",
    "base_model = base_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3)"
   ],
   "id": "fbedb141604afab2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:32<00:00, 46.99it/s, loss=0.406, val_acc=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:33:33.345423Z",
     "start_time": "2025-06-02T15:33:30.375879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training base model accuracy\", evaluate_accuracy(train_dataset, base_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation base model accuracy\", evaluate_accuracy(val_dataset, base_model, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "b692be0391f31301",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model accuracy 0.9325999617576599\n",
      "Validation base model accuracy 0.19429999589920044\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Sam\n",
    "-------------------------\n",
    "\n",
    "rho = 0.05 (default)"
   ],
   "id": "3d2532908864df1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:37:41.494993Z",
     "start_time": "2025-06-02T15:33:45.884597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sam_model = get_model()\n",
    "\n",
    "sam_trainer = SAMTrainer(sam_model, device=DEVICE)\n",
    "sam_model = sam_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.05)"
   ],
   "id": "7728800ee10035ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:55<00:00, 42.45it/s, loss=0.987, val_acc=0.0781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:19:14.922911Z",
     "start_time": "2025-06-02T16:19:11.965003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training SAM model accuracy\", evaluate_accuracy(train_dataset, sam_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy\", evaluate_accuracy(val_dataset, sam_model, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "c43aa78adab78017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy 0.9426999688148499\n",
      "Validation SAM model accuracy 0.18490000069141388\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "rho = 0.01 (smaller)",
   "id": "b9c453579e3c12f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:10:30.526015Z",
     "start_time": "2025-06-02T21:06:33.829528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sam_model_1 = get_model()\n",
    "\n",
    "sam_trainer_1 = SAMTrainer(sam_model_1, device=DEVICE)\n",
    "sam_model_1 = sam_trainer_1.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.01)"
   ],
   "id": "97c87129620d8e8e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:56<00:00, 42.26it/s, loss=0.512, val_acc=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:10:33.704521Z",
     "start_time": "2025-06-02T21:10:30.595925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training SAM model accuracy (rho=0.01)\", evaluate_accuracy(train_dataset, sam_model_1, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy (rho=0.01)\", evaluate_accuracy(val_dataset, sam_model_1, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "e9c677aa353e57d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy (rho=0.01) 0.9380999803543091\n",
      "Validation SAM model accuracy (rho=0.01) 0.19029998779296875\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "rho = 0.001 (way smaller)",
   "id": "a5bbc14c993c9365"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:14:30.830365Z",
     "start_time": "2025-06-02T21:10:33.713736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sam_model_2 = get_model()\n",
    "\n",
    "sam_trainer_2 = SAMTrainer(sam_model_2, device=DEVICE)\n",
    "sam_model_2 = sam_trainer_2.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.001)"
   ],
   "id": "27eea8974f740326",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:57<00:00, 42.18it/s, loss=0.373, val_acc=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:14:33.870277Z",
     "start_time": "2025-06-02T21:14:30.872448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training SAM model accuracy (rho=0.001)\", evaluate_accuracy(train_dataset, sam_model_2, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy (rho=0.001)\", evaluate_accuracy(val_dataset, sam_model_2, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "86e1c1038b13c8c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy (rho=0.001) 0.9375999569892883\n",
      "Validation SAM model accuracy (rho=0.001) 0.18809999525547028\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "rho = 0.1 (bigger)",
   "id": "e244c74e596bbc5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:18:30.776437Z",
     "start_time": "2025-06-02T21:14:33.879737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sam_model_3 = get_model()\n",
    "\n",
    "sam_trainer_3 = SAMTrainer(sam_model_3, device=DEVICE)\n",
    "sam_model_3 = sam_trainer_3.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.1)"
   ],
   "id": "51d987135c1bf3b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:56<00:00, 42.22it/s, loss=2.16, val_acc=0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  2 ----------\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:18:52.039792Z",
     "start_time": "2025-06-02T21:18:48.978488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training SAM model accuracy (rho=0.1)\", evaluate_accuracy(train_dataset, sam_model_3, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SAM model accuracy (rho=0.1)\", evaluate_accuracy(val_dataset, sam_model_3, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "42ea1ca335c85a6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SAM model accuracy (rho=0.1) 0.7524999976158142\n",
      "Validation SAM model accuracy (rho=0.1) 0.18159998953342438\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vanilla SGD vs SAM + SGD\n",
    "-----------------------------\n",
    "\n",
    "Not modularized yet (changed manually adam -> sgd in SamTrainer and SimpleTrainer :/ )"
   ],
   "id": "28540ed2893ccb3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:24:15.737721Z",
     "start_time": "2025-06-02T21:20:20.966826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sam_sgd_model = get_model()\n",
    "\n",
    "sam_sgd_trainer = SAMTrainer(sam_sgd_model, device=DEVICE)\n",
    "sam_sgd_model = sam_sgd_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3, rho=0.01)"
   ],
   "id": "47bc3d3d1553ee78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:54<00:00, 42.60it/s, loss=4.6, val_acc=0.0312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  Training completed with loss  5 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:24:18.792746Z",
     "start_time": "2025-06-02T21:24:15.772268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training SGD SAM model accuracy (rho=0.01)\", evaluate_accuracy(train_dataset, sam_sgd_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation SGD SAM model accuracy (rho=0.01)\", evaluate_accuracy(val_dataset, sam_sgd_model, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "7b68f2c66eee16a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD SAM model accuracy (rho=0.01) 0.012600000016391277\n",
      "Validation SGD SAM model accuracy (rho=0.01) 0.013799999840557575\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:33:11.281235Z",
     "start_time": "2025-06-02T21:32:49.265386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = get_model()\n",
    "\n",
    "base_trainer = SimpleTrainer(base_model, device=DEVICE)\n",
    "base_model = base_trainer.train(train_dataset, val_dataset, loss_obj=0.000000000000001, max_iters=10000, batch_size=64, lr=1e-3)"
   ],
   "id": "57c2e1f7a0427dc7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1058/10000 [00:21<03:04, 48.43it/s, loss=4.61, val_acc=0.0156]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[79]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m base_model = get_model()\n\u001B[32m      3\u001B[39m base_trainer = SimpleTrainer(base_model, device=DEVICE)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m base_model = \u001B[43mbase_trainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.000000000000001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iters\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:5\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/Trainer.py:30\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001B[39m\n\u001B[32m     28\u001B[39m     X, y = \u001B[38;5;28mnext\u001B[39m(data_iter)\n\u001B[32m     29\u001B[39m X, y = X.to(\u001B[38;5;28mself\u001B[39m._device), y.to(\u001B[38;5;28mself\u001B[39m._device)\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m loss, info_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m progress_bar.set_postfix({\n\u001B[32m     32\u001B[39m                              \u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mround\u001B[39m(loss, \u001B[32m4\u001B[39m),\n\u001B[32m     33\u001B[39m                          } | info_dict)\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m loss < loss_obj:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/SimpleTrainer.py:48\u001B[39m, in \u001B[36mSimpleTrainer.step\u001B[39m\u001B[34m(self, X, y, lr, **kwargs)\u001B[39m\n\u001B[32m     46\u001B[39m loss.backward()\n\u001B[32m     47\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer.step()\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m accuracy = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluate_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.item(), {\u001B[33m\"\u001B[39m\u001B[33mval_acc\u001B[39m\u001B[33m\"\u001B[39m:\u001B[38;5;28mround\u001B[39m(accuracy, \u001B[32m4\u001B[39m)}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/SimpleTrainer.py:36\u001B[39m, in \u001B[36mSimpleTrainer._evaluate_accuracy\u001B[39m\u001B[34m(self, num_samples)\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;28mself\u001B[39m._model.eval()\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mevaluate_accuracy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_val_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_device\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/utils/evaluation.py:8\u001B[39m, in \u001B[36mevaluate_accuracy\u001B[39m\u001B[34m(dataset, model, num_samples, device)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mevaluate_accuracy\u001B[39m(dataset: torch.utils.data.Dataset, model: torch.nn.Sequential, num_samples: \u001B[38;5;28mint\u001B[39m = \u001B[32m2000\u001B[39m,\n\u001B[32m      7\u001B[39m                       device=\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m) -> \u001B[38;5;28mfloat\u001B[39m:\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     X, y = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mutils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     X, y = X.to(device), y.to(device)\n\u001B[32m     10\u001B[39m     logit = model(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/datasets/cifar.py:119\u001B[39m, in \u001B[36mCIFAR10.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    116\u001B[39m img = Image.fromarray(img)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     img = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    122\u001B[39m     target = \u001B[38;5;28mself\u001B[39m.target_transform(target)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001B[39m, in \u001B[36mNormalize.forward\u001B[39m\u001B[34m(self, tensor)\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) -> Tensor:\n\u001B[32m    270\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    271\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    272\u001B[39m \u001B[33;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    275\u001B[39m \u001B[33;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[32m    276\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001B[39m, in \u001B[36mnormalize\u001B[39m\u001B[34m(tensor, mean, std, inplace)\u001B[39m\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch.Tensor):\n\u001B[32m    348\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m350\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:928\u001B[39m, in \u001B[36mnormalize\u001B[39m\u001B[34m(tensor, mean, std, inplace)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m std.ndim == \u001B[32m1\u001B[39m:\n\u001B[32m    927\u001B[39m     std = std.view(-\u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[43m.\u001B[49m\u001B[43msub_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdiv_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Lagrangian\n",
    "---------------------"
   ],
   "id": "1421b9889cf7788c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:32:41.003512Z",
     "start_time": "2025-06-02T16:31:45.808228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lagrangian_model = get_model()\n",
    "lagrangian_trainer = LagrangianTrainer(lagrangian_model, LogVolume(epsilon=1e-12), device=DEVICE)\n",
    "lagrangian_trainer.set_volume_constrain(1e-4) # start with a small volume at first\n",
    "print(lagrangian_trainer._volume_function(lagrangian_trainer._interval_model))\n",
    "lagrangian_trainer.train(\n",
    "    train_dataset, val_dataset, loss_obj=-0.000000000000001, max_iters=3000, batch_size=64, lr=1e-4\n",
    ")"
   ],
   "id": "49eb57ecd2bffc9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-8.5172, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1060/3000 [00:54<01:40, 19.30it/s, loss=4.6, min_val_acc=0, current_volume=-6.9]     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[57]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m lagrangian_trainer.set_volume_constrain(\u001B[32m1e-4\u001B[39m) \u001B[38;5;66;03m# start with a small volume at first\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(lagrangian_trainer._volume_function(lagrangian_trainer._interval_model))\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43mlagrangian_trainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m0.000000000000001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iters\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-4\u001B[39;49m\n\u001B[32m      7\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:16\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:33\u001B[39m, in \u001B[36mConstrainedVolumeTrainer.train\u001B[39m\u001B[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28mself\u001B[39m._interval_model.train()\n\u001B[32m     32\u001B[39m \u001B[38;5;28mself\u001B[39m._current_val_dataset=val_dataset\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss_obj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mloss_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_iters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/Trainer.py:30\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, train_dataset, val_dataset, loss_obj, max_iters, batch_size, lr, **kwargs)\u001B[39m\n\u001B[32m     28\u001B[39m     X, y = \u001B[38;5;28mnext\u001B[39m(data_iter)\n\u001B[32m     29\u001B[39m X, y = X.to(\u001B[38;5;28mself\u001B[39m._device), y.to(\u001B[38;5;28mself\u001B[39m._device)\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m loss, info_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m progress_bar.set_postfix({\n\u001B[32m     32\u001B[39m                              \u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mround\u001B[39m(loss, \u001B[32m4\u001B[39m),\n\u001B[32m     33\u001B[39m                          } | info_dict)\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m loss < loss_obj:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:59\u001B[39m, in \u001B[36mConstrainedVolumeTrainer.step\u001B[39m\u001B[34m(self, X, y, lr, **kwargs)\u001B[39m\n\u001B[32m     57\u001B[39m \u001B[38;5;28mself\u001B[39m._interval_model.train()\n\u001B[32m     58\u001B[39m loss, infos = \u001B[38;5;28mself\u001B[39m._optimize_step(X, y, lr=lr, **kwargs)\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m min_acc = \u001B[38;5;28mround\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_evaluate_min_val_acc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_current_val_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[32m4\u001B[39m)\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss, {\u001B[33m\"\u001B[39m\u001B[33mmin_val_acc\u001B[39m\u001B[33m\"\u001B[39m: min_acc} | infos\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/cours/MA2/Opti/project/src/optimizers/ConstrainedVolumeTrainer.py:46\u001B[39m, in \u001B[36mConstrainedVolumeTrainer._evaluate_min_val_acc\u001B[39m\u001B[34m(self, val_dataset, num_samples)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_evaluate_min_val_acc\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[32m     44\u001B[39m                           val_dataset: torch.utils.data,\n\u001B[32m     45\u001B[39m                           num_samples: \u001B[38;5;28mint\u001B[39m = \u001B[32m64\u001B[39m) -> \u001B[38;5;28mfloat\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m     X, y = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mutils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m     \u001B[38;5;28mself\u001B[39m._interval_model.eval()\n\u001B[32m     48\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/datasets/cifar.py:119\u001B[39m, in \u001B[36mCIFAR10.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    116\u001B[39m img = Image.fromarray(img)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     img = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    122\u001B[39m     target = \u001B[38;5;28mself\u001B[39m.target_transform(target)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001B[39m, in \u001B[36mNormalize.forward\u001B[39m\u001B[34m(self, tensor)\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) -> Tensor:\n\u001B[32m    270\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    271\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    272\u001B[39m \u001B[33;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    275\u001B[39m \u001B[33;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[32m    276\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m277\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001B[39m, in \u001B[36mnormalize\u001B[39m\u001B[34m(tensor, mean, std, inplace)\u001B[39m\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch.Tensor):\n\u001B[32m    348\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m350\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/libraries/miniconda3/envs/opti/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:928\u001B[39m, in \u001B[36mnormalize\u001B[39m\u001B[34m(tensor, mean, std, inplace)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m std.ndim == \u001B[32m1\u001B[39m:\n\u001B[32m    927\u001B[39m     std = std.view(-\u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m928\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[43m.\u001B[49m\u001B[43msub_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m)\u001B[49m.div_(std)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:25:07.502587Z",
     "start_time": "2025-06-02T16:25:04.468891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training base model accuracy\",\n",
    "      evaluate_accuracy(train_dataset, lagrangian_model, num_samples=len(val_dataset), device=DEVICE))\n",
    "print(\"Validation base model accuracy\",\n",
    "      evaluate_accuracy(val_dataset, lagrangian_model, num_samples=len(val_dataset), device=DEVICE))"
   ],
   "id": "3741be3d426b6981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base model accuracy 0.010300000198185444\n",
      "Validation base model accuracy 0.009999999776482582\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
